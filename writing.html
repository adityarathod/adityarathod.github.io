<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>Aditya Rathod | Writing</title><meta name="robots" content="index,follow"/><meta name="description" content="Musings on tech, life, and whatever else comes to mind."/><meta property="og:title" content="Aditya Rathod | Writing"/><meta property="og:description" content="Musings on tech, life, and whatever else comes to mind."/><meta name="next-head-count" content="7"/><link rel="preload" href="/_next/static/css/dd67a8de90d82862.css" as="style"/><link rel="stylesheet" href="/_next/static/css/dd67a8de90d82862.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-8fa1640cc84ba8fe.js" defer=""></script><script src="/_next/static/chunks/framework-3b5a00d5d7e8d93b.js" defer=""></script><script src="/_next/static/chunks/main-b6ae7f85f00af153.js" defer=""></script><script src="/_next/static/chunks/pages/_app-1b961b81afafe8a2.js" defer=""></script><script src="/_next/static/chunks/pages/writing-661f051007c328ec.js" defer=""></script><script src="/_next/static/3ceUdyat16U65wocjUq8G/_buildManifest.js" defer=""></script><script src="/_next/static/3ceUdyat16U65wocjUq8G/_ssgManifest.js" defer=""></script></head><body><div id="__next"><section><div class="w-full bg-cyan h-[3px]"></div><header class="mt-3 px-8 py-3 mx-auto flex flex-row items-center justify-between sm:justify-between md:justify-start lg:justify-start xl:justify-start"><div class="mr-0 sm:mr-0 md:mr-8 lg:mr-10 xl:mr-10 text-center"><a class="font-bold text-2xl text-white opacity-100 hover:opacity-80" href="/">Aditya Rathod<span class="text-spring">.</span></a></div><div class="flex-1"></div><nav class="hidden sm:hidden md:flex lg:flex xl:flex flaex-1 flex-row items-center"><a class="text-white text-md opacity-80 hover:opacity-100 transition-opacity ml-10" href="/#projects">Projects</a><a class="text-white text-md opacity-80 hover:opacity-100 transition-opacity ml-10" href="/writing">Writing</a><a href="/resume.pdf" class="text-white text-md opacity-80 hover:opacity-100 transition-opacity ml-10">Resume</a></nav></header><nav class="mt-2 pl-8 flex sm:flex md:hidden flex-1 flex-row items-a"><a class="text-white text-md opacity-80 hover:opacity-100 transition-opacity mr-10" href="/#projects">Projects</a><a class="text-white text-md opacity-80 hover:opacity-100 transition-opacity mr-10" href="/writing">Writing</a><a href="/resume.pdf" class="text-white text-md opacity-80 hover:opacity-100 transition-opacity mr-10">Resume</a></nav></section><div class="py-10 px-8 max-w-6xl"><article><main><h1 class="text-4xl font-bold mb-4">Writing</h1><div class="py-8"><h4 class="text-sm font-semibold text-cyan uppercase" role="doc-subtitle">August 4, 2018</h4><a href="/blog/predicting-titanic-survivors"><div class="hover:opacity-70 transition-opacity"><h2 class="text-2xl text-white font-bold">Predicting Titanic Survivors: My First Machine Learning Model</h2><p class="text-md mt-2 opacity-80 text-white">Learn how to build a machine learning model to predict Titanic survivors based on the Kaggle dataset.</p></div></a><p class="mt-3"><a class="inline mr-2 py-1 px-2 rounded-sm bg-gray-800 text-white text-xs hover:text-gray-300" href="/writing?tag=Legacy%20Content">Legacy Content</a></p></div><div class="py-8"><h4 class="text-sm font-semibold text-cyan uppercase" role="doc-subtitle">December 16, 2017</h4><a href="/blog/protect-trivia-from-bots"><div class="hover:opacity-70 transition-opacity"><h2 class="text-2xl text-white font-bold">Protecting Trivia Apps from Cheaters</h2><p class="text-md mt-2 opacity-80 text-white">How people cheat at trivia games, and how the companies running them could stop this from happening.</p></div></a><p class="mt-3"><a class="inline mr-2 py-1 px-2 rounded-sm bg-gray-800 text-white text-xs hover:text-gray-300" href="/writing?tag=Legacy%20Content">Legacy Content</a></p></div><div class="py-8"><h4 class="text-sm font-semibold text-cyan uppercase" role="doc-subtitle">November 12, 2017</h4><a href="/blog/analyzing-browser-hist-using-python"><div class="hover:opacity-70 transition-opacity"><h2 class="text-2xl text-white font-bold">Analyzing Browser History Using Python and Pandas</h2><p class="text-md mt-2 opacity-80 text-white">Explore the depths of your browser history with this introductory data science tutorial.</p></div></a><p class="mt-3"><a class="inline mr-2 py-1 px-2 rounded-sm bg-gray-800 text-white text-xs hover:text-gray-300" href="/writing?tag=Legacy%20Content">Legacy Content</a></p></div></main></article></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"content":"\nToday, we are going to do some spelunking within the deep, dark place which is your browser history.\nIn order to obtain the data for this tutorial from Google Chrome, go to\n`~/Library/Application Support/Google/Chrome/Default`\non a Mac/Linux computer or `%LocalAppData%\\Google\\Chrome\\User Data\\Default` on a Windows PC.\nRun the following SQLite command to obtain a text file in reverse chronological order:\n\n\u003cMultiCode\n  tabs={[\n    {\n      name: \"Mac/Linux\",\n      content: `sqlite3 History \"select datetime(last_visit_time/1000000-11644473600,'unixepoch'), url \\\\\nfrom urls order by last_visit_time desc\" \u003e ~/hist.txt`,\n      language: \"bash\",\n    },\n    {\n      name: \"Windows\",\n      content: `sqlite3 History \"select datetime(last_visit_time/1000000-11644473600,'unixepoch'), url \\\\\nfrom urls order by last_visit_time desc\" \u003e %userprofile%\\hist.txt`,\n      language: \"bash\",\n    },\n  ]}\n/\u003e\n\nCheck your user folder. A file called `hist.txt` should be there. Move the file to a suitable place for this exercise.\n\n(This process brought to you by the brilliant people on [Stack Exchange](https://superuser.com/questions/602252/can-chrome-browser-history-be-exported-to-an-html-file))\n\nImport the needed libraries, `numpy` and `pandas`:\n\n\u003cMultiCode\n  tabs={[\n    {\n      name: \"main.py\",\n      content: `import pandas as pd\nimport numpy as np`,\n    },\n  ]}\n/\u003e\n\n## Clean Up Data\n\nThat data that we pulled is extremely messy. Here's an example row:\n\n```wiki\n2017-11-12 21:10:11|https://news.ycombinator.com/item?id=15678587\n```\n\nWe need to split on that vertical bar while making sure not to split on a bar in the URL itself.\nSince Pandas probably doesn't do this out of the box, let's write a custom import function:\n\n\u003cMultiCode\n  tabs={[\n    {\n      name: \"main.py\",\n      content: `# ...continued from previous code block...\n# Open our file\nwith open('hist.txt') as f:\n    content = f.readlines()\n# Strip whitespace then split on first occurrence of pipe character\nraw_data = [line.split('|', 1) for line in [x.strip() for x in content]]\n# We now have a 2D list.\nprint(raw_data[1])`,\n      language: \"python\",\n    },\n    {\n      name: \"Result\",\n      content: `['2017-11-12 21:09:21', 'https://news.ycombinator.com/']`,\n      language: \"wiki\",\n    },\n  ]}\n/\u003e\n\nUsing our 2D list, let's make a Pandas DataFrame with custom column headers and make sure it is working:\n\n\u003cMultiCode\n  tabs={[\n    {\n      name: \"main.py\",\n      content: `# ...continued from previous code block...\ndata = pd.DataFrame(raw_data, columns=['datetime', 'url'])\ndata.head(1)`,\n      language: \"python\",\n    },\n  ]}\n/\u003e\n\n| \u0026emsp; | datetime            | url                                           |\n| ------ | ------------------- | --------------------------------------------- |\n| 0      | 2017-11-12 21:10:11 | https://news.ycombinator.com/item?id=15678587 |\n\nNow, we're almost done with ingesting the data. Let's convert the datetime string column into a\ncolumn of Pandas `datetime` elements and double-check that it is indeed a Pandas timestamp:\n\n\u003cMultiCode\n  tabs={[\n    {\n      name: \"main.py\",\n      content: `# ...continued from previous code block...\ndata.datetime = pd.to_datetime(data.datetime)\ndata.datetime[0]`,\n      language: \"python\",\n    },\n  ]}\n/\u003e\n\nFinally, let's remove all information from the URL, leaving only the domain/subdomain and check our work again:\n\n\u003cMultiCode\n  tabs={[\n    {\n      name: \"main.py\",\n      content: `# ...continued from previous code block...\nfrom urllib.parse import urlparse\nparser = lambda u: urlparse(u).netloc\ndata.url = data.url.apply(parser)\ndata.head(1)`,\n      language: \"python\",\n    },\n  ]}\n/\u003e\n\n| \u0026emsp; | datetime            | url                  |\n| ------ | ------------------- | -------------------- |\n| 0      | 2017-11-12 21:10:11 | news.ycombinator.com |\n\nFinally, our data is clean.\n\n## Analyzing the Data\n\nNow that the boring part is done, let's analyze our browsing data.\n\n### Most Visited Sites\n\nLet's generate a list of our top sites sorted by frequency, then print out the first two to get our most visited sites:\n\n\u003cMultiCode\n  tabs={[\n    {\n      name: \"main.py\",\n      content: `# ...continued from previous code block...\n# Aggregate domain entries\nsite_frequencies = data.url.value_counts().to_frame()\n# Make the domain a column\nsite_frequencies.reset_index(level=0, inplace=True)\n# Rename columns to appropriate names\nsite_frequencies.columns = ['domain', 'count']\n# Display top 2\nsite_frequencies.head(2)`,\n      language: \"python\",\n    },\n  ]}\n/\u003e\n\n| \u0026emsp; | domain         | count |\n| ------ | -------------- | ----- |\n| 0      | www.google.com | 3904  |\n| 1      | github.com     | 1571  |\n\nIt should come as no shock that my top sites, just like any other dev, were Google and Github.\n\n_(Is secretly surprised that Stack Overflow was not one of the top ten)_\n\nNow, let's see our top sites in a beautiful chart:\n\n\u003cMultiCode\n  tabs={[\n    {\n      name: \"main.py\",\n      content: `# ...continued from previous code block...\nimport matplotlib.pyplot as plt\ntopN = 20\nplt.figure(1, figsize=(10,10))\nplt.title('Top $n Sites Visited'.replace('$n', str(topN)))\npie_data = site_frequencies['count'].head(topN).tolist()\npie_labels = None\n# Uncomment to get specific domain names\n# pie_labels = site_frequencies['domain'].head(topN).tolist()\nplt.pie(pie_data, autopct='%1.1f%%', labels=pie_labels)\nplt.show()`,\n      language: \"python\",\n    },\n  ]}\n/\u003e\n\n\u003cImage\n  src=\"/post-images/browser-hist-graph.jpg\"\n  width={500}\n  height={500}\n  className=\"mx-auto\"\n  alt=\"browser history graph\"\n/\u003e\n\nWhat else can we do with the data? A lot, I'm sure, since this is the Holy Grail for ad tracking\ncompanies.\n","data":{"title":"Analyzing Browser History Using Python and Pandas","description":"Explore the depths of your browser history with this introductory data science tutorial.","date":1510516800000,"displayDate":"November 12, 2017","tags":["Legacy Content"]},"filePath":"analyzing-browser-hist-using-python.mdx"},{"content":"\nI've been taking the [Machine Learning course by Dr. Andrew Ng](https://www.coursera.org/learn/machine-learning) online through Coursera, and so far it's been an enlightening and enjoyable experience. While I've been taking lots of notes on the equations and implementation details on linear and logistic regression models, I thought it would be a fun weekend exercise to implement these algorithms from scratch using my favorite toolkit: Python, Pandas, and Numpy.\n\nSpecial thanks to Dr. Ng and his course staff for preparing such high-quality course material. Many of these equations and explanations are based on notes from the course lectures, and I have paraphrased many of the explanations here in my own words as best as possible.\n\n## The Dataset\n\nThe dataset I decided to work with for this project was the was the [Kaggle Titanic dataset](https://www.kaggle.com/c/titanic), which was recommended as a beginner dataset to work with.\n\n![Titanic Dataset Data](/post-images/titanic-training.png)\n\n\u003ccenter\u003e\n  \u003cem\u003e\n    \u003cstrong\u003eFigure 1:\u003c/strong\u003e Structure of the dataset\n  \u003c/em\u003e\n\u003c/center\u003e\n\nBased on the `train.csv` file included in the dataset download, it was clear that this was a binary classification problem. Since I only know how to implement linear and logistic regression models so far, I decided to use logistic regression for this problem.\n\n## Project Structure\n\nFor reference, my project directory was structured like this:\n\n```text\n.\n├── Makefile\n├── data\n│   ├── gender_submission.csv\n│   ├── test.csv\n│   └── train.csv\n└── src\n    └── Titanic.ipynb\n```\n\nThe `Makefile` in the main directory has a fake target to make launching `jupyter lab` easier.\n\n## Preparing the Data\n\nI first needed to clean the data before I could use it. In order to do that, I first brought the data into a pandas `DataFrame`:\n\n```python\nimport pandas as pd\ndf = pd.read_csv(\"../data/train.csv\")\n```\n\nThen, I dropped the columns that I couldn't convert into numbers:\n\n```python\ndf = df.drop(columns=['Name', 'PassengerId', 'Ticket', 'Cabin'])\n```\n\n**Note:** Having fewer features means the model was less accurate than it could be. Some clever feature engineering would have made use of these columns somehow, but as I was a beginner, I didn't really know what to do with these features other than throw them out.\n\nNext, I converted all of the textual features into numerical values roughly within the range $(-1, 1)$:\n\n```python\ndef sex_to_numerical(s):\n    return int(g == \"female\")\ndef embarkation_to_numerical(e):\n    if (e == \"C\"):\n        return -1\n    elif (e == \"Q\"):\n        return 0\n    else:\n        return 1\ndf.Sex = df.Sex.apply(sex_to_numerical)\ndf.Embarked = df.Embarked.apply(embarkation_to_numerical)\n```\n\nI then normalized the data using a simple approach: $(\\text{value} - \\text{max})/\\text{range}$\nwhere $\\text{range} = \\text{max} - \\text{min}$.\n\n```python\ndef normalize_ticket_class(cl):\n    return (cl - 3) / 2\ndef normalize_fare(farecol):\n    range_f = farecol.max() - farecol.min()\n    max_f = farecol.max()\n    return farecol.apply(lambda fare: (fare - max_f)/range_f)\ndf.Fare = normalize_fare(df.Fare)\ndf.Pclass = df.Pclass.apply(normalize_ticket_class)\n```\n\nI needed to convert all missing/`NaN` values in the `Age` column to numbers so that I didn't need to throw that feature out:\n\n```python\ndef de_nan_age(a):\n    if (np.isnan(a)):\n        return 0.0\n    else:\n        return a\ndf.Age = df.Age.apply(de_nan_age)\n```\n\nAfter all this processing, the `DataFrame` looks like this:\n\n| Index | Survived | Pclass | Sex | Age  | SibSp | Parch | Fare                | Embarked |\n| ----- | -------- | ------ | --- | ---- | ----- | ----- | ------------------- | -------- |\n| 0     | 0        | 0.0    | 0   | 22.0 | 1     | 0     | -0.985848942437792  | 1        |\n| 1     | 1        | -1.0   | 1   | 38.0 | 1     | 0     | -0.8608642646173593 | -1       |\n| 2     | 1        | 0.0    | 1   | 26.0 | 0     | 0     | -0.9845314301820002 | 1        |\n| 3     | 1        | -1.0   | 1   | 35.0 | 1     | 0     | -0.8963557025443796 | 1        |\n| 4     | 0        | 0.0    | 0   | 35.0 | 0     | 0     | -0.9842874464309276 | 1        |\n\nNow, I'll split up the `DataFrame` into two `np.array`s, one with the features, the other with the labels:\n\n```python\nfeatures = df.drop(columns=['Survived']).values\nlabels = df.Survived.values\n```\n\nAdd an $x_0 = 1$ column to the features array so everything looks clean:\n\n```python\nx_zero_column = np.zeros((features.shape[0], 1)) + 1\nfeatures = np.append(x_zero_column, features, axis=1)\n```\n\nUsing the features and labels arrays, create a _design matrix_ $X$ and the _label vector_ $y$:\n\n```python\nX = np.asmatrix(features)\n# y needs to be transposed from a row vector to a column vector\ny = np.asmatrix(labels).T\n```\n\nThe data is now ready to be used.\n\n## Define Variables\n\nUsing $X$ and $y$:, it's time to define some variables that are important to the implmentation of the _cost function_ $J$:\n\n```python\n# m is the number of training samples\nm = X.shape[0]\n# n is the number of features, in this case we are not counting the x_0 column\nn = X.shape[1] - 1\n```\n\nLet's define the parameter $\\theta$ as a $n+1$-dimensional vector, initially filled with zeros:\n\n```python\ntheta = np.asmatrix(np.zeros(n + 1)).T\n```\n\nAnd let's set a learning rate $\\alpha$, in this case I just chose an arbitrary value:\n\n```python\nalpha = 0.001\n```\n\nIt's important to choose the right $\\alpha$, because a smaller value may make the model slower to converge, while a larger value might make the model never converge.\n\n## Define Functions\n\n### The Sigmoid Function\n\nOne of the most important equations needed for logistic regression is the sigmoid function, which looks like this:\n\n\u003ccenter\u003e\n  {/* \u003cDesmos url=\"https://www.desmos.com/calculator/hvlhutwrba?embed\" /\u003e */}\n\u003c/center\u003e\n\nThe equation for the sigmoid function is\n\n$$\ng(z) = \\frac{1}{1+ e^{-z}}\n$$\n\nThanks to numpy, it's very simple to define an element-wise vectorized sigmoid function, which will our code much more efficient:\n\n```python\ndef sigmoid(z):\n    t = np.exp(-z) + 1\n    return np.divide(1, t)\n```\n\n### The Hypothesis Function\n\nTo generate a matrix of predictions using the parameter vector $\\theta$, we define our hypothesis function $h_{\\theta}(x)$ as follows:\n\n$$\nh_{\\theta}(x) = g(X\\theta)\n$$\n\nwhere $X$ is the design matrix.\n\nBut what does the output of this function mean? $h_{\\theta}(x)$ is actually outputting the _probability_ that the output is 1.\nFormally, this function gives you the probablility that $y=1$, given $x$, parameterized by $\\theta$ (mathematically, $P(y=1|x;\\theta)$).\n\nIn Python, we can define the hypothesis function similarly:\n\n```python\ndef hypothesis(x):\n    return sigmoid(x * theta)\n```\n\n### The Cost Function\n\nIn order for the logistic regression algorithm to _learn_, it must have some function to minimize. This function is called the _cost_ or _loss_ function.\n\nFor logistic regression, the cost function is defined as:\n\n$$\nJ(\\theta) = \\frac{1}{m} \\bigg(-y^T \\log{h(x)}- (1-y^T)\\log{(1-h(x))}\\bigg)\n$$\n\nThe Python code for this function came out to be too long to write out on one line, so the math is done on three separate lines.\n\n**Note:** `cost` came out to be a 1x1 numpy matrix and was converted into a singular floating-point value at the end.\n\n```python\ndef cost():\n    hypo = hypothesis(X)\n    part1 = -(y.T) * np.log(hypo)\n    part2 = (1-(y.T)) * np.log(1 - hypo)\n    cost = (1/m) * (part1 - part2)\n    return cost[0,0]\n```\n\n### The Gradient Descent Function\n\nThe last (and probably most important) piece of this machine learning algorithm is the `gradient_descent_step` function, which calculates the gradient for $\\theta$.\n\nThe update rule for $\\theta$ is as follows:\n\n$$\n\\theta := \\theta - \\frac{\\alpha}{m}X^T(g(X\\theta)-\\vec{y})\n$$\n\nThis is simple to implement in Python as a one-liner:\n\n```python\ndef gradient_descent_step(theta):\n    return theta - (alpha/m)*(X.T)*(hypothesis(X) - y)\n```\n\nWith all the necessary functions implemented, it's time to actually learn the parameters $\\theta$.\n\n## Run the Learning Algorithm\n\nTo run the learning algorithm, let's create a loop in which `gradient_descent_step` is run every iteration:\n\n```python\nfor i in range(1630000):\n    theta = gradient_descent_step(theta)\n```\n\nI found experimentally that after 1.63 million iterations, the cost delta after each iteration of Gradient Descent is negligible.\n\nAfter the loop was run, the cost was approximately `0.45085533814736584`.\n\nBefore we do anything else, let's save the optimal $\\theta$ in a file:\n\n```python\nnp.savetxt(\"theta.txt\", theta)\n```\n\nOnce saved, it can be loaded back like this:\n\n```python\ntheta = np.asmatrix(np.loadtxt(\"theta.txt\"))\ntheta = theta.T\n```\n\n## Run the Model on Test Data\n\n### Create the Prediction Function\n\nUsing the `hypothesis(x)` function, let's define a function that outputs `1` if $P(y=1|x;\\theta) ≥ 0.5$ or `0` otherwise:\n\n```python\ndef predict(x):\n    h = hypothesis(x)\n    comp = (h \u003e= 0.5)\n    return comp.astype(int)\n```\n\n`predict(x)` runs predictions on a matrix element-wise and converts them into numerical binary values.\n\n### Import the Test Data\n\nNow, it's time to bring in the test data, preparing it like we did for the training data:\n\n```python\ntest_df = pd.read_csv(\"../data/test.csv\")\ntest_df = test_df.drop(columns=['Name', 'PassengerId', 'Ticket', 'Cabin'])\ntest_df.Pclass = test_df.Pclass.apply(normalize_ticket_class)\ntest_df.Sex = test_df.Sex.apply(sex_to_numerical)\ntest_df.Fare = normalize_fare(test_df.Fare)\ntest_df.Embarked = test_df.Embarked.apply(embarkation_to_numerical)\ntest_df.Age = test_df.Age.apply(de_nan_age)\n```\n\nLet's add in the $x_0$ column like before:\n\n```python\ntest_X = test_df.values\nx_zero_column_test = np.zeros((test_df.shape[0], 1)) + 1\ntest_X = np.append(x_zero_column_test, test_X, axis=1)\ntest_X = np.asmatrix(test_X)\n```\n\n### Generate Predictions\n\nUsing the `test_X` matrix, let's generate predictions and save them to a file:\n\n```python\npredictions = predict(test_X)\nnp.savetxt(\"predictions.txt\", predictions)\n```\n\n### Prepare Submission For Kaggle\n\nThe `predictions.txt` file isn't in the format that Kaggle needs for submission. Let's put our predictions into a pandas `DataFrame` along with the `PassengerId` column:\n\n```python\n# Function to flatten nested lists\nflatten = lambda l: [item for sublist in l for item in sublist]\n# Temp DataFrame to get PassengerIds\ntemp_df = pd.read_csv(\"../data/test.csv\")\nsubmission_df = pd.DataFrame()\nsubmission_df[\"PassengerId\"] = temp_df[\"PassengerId\"]\nsubmission_df[\"Survived\"] = pd.Series(flatten(predictions.flatten().tolist()))\n```\n\nLet's export to CSV for upload to Kaggle:\n\n```python\nsubmission_df.to_csv(\"kaggle_submission.csv\", index=False)\n```\n\n## Results\n\nAccording to Kaggle's online judge, my model had an accuracy of 74.641% on the test dataset, which is not great by machine learning standards, but was nevertheless exciting for me, since I implemented the model (almost) from scratch.\n\nThis was an improvement over my submission from last year, which was a linear regression algorithm using `scikit-learn`.\n\n![Kaggle leaderboard screenshot](/post-images/kaggle-score-logistic.png)\n\n\u003ccenter\u003e\n  \u003cem\u003e\n    \u003cstrong\u003eFigure 2:\u003c/strong\u003e Pics or it didn't happen\n  \u003c/em\u003e\n\u003c/center\u003e\n\n## Conclusion\n\nThis project was also a chance to dip my toes into creating Machine Learning models for use on real data, taking the theory and applying it to a real-world problem.\n\nBased on my results, I can also conclude that feature engineering (such as using polynomial features) may be needed to achieve a higher accuracy. Additionally, using something like neural networks may be a better option to achieve higher accuracy, due to their ability to learn complex nonlinear hypotheses.\n\nIf you are a machine learning expert and have suggestions on how I could improve the accuracy of my toy model, I would love to hear them! Comment on the Hacker News thread linked below or email me at the address below.\n\n[Discuss on Hacker News](https://news.ycombinator.com/item?id=17692378)\n","data":{"title":"Predicting Titanic Survivors: My First Machine Learning Model","description":"Learn how to build a machine learning model to predict Titanic survivors based on the Kaggle dataset.","displayDate":"August 4, 2018","date":1533423600000,"tags":["Legacy Content"]},"filePath":"predicting-titanic-survivors.mdx"},{"content":"\n_This article has been mentioned in the media: [The Verge](https://www.theverge.com/2017/12/25/16817502/how-to-stop-hq-cheaters), [Washington Post](https://www.washingtonpost.com/graphics/2018/business/hq-trivia/), and [TechCrunch](https://techcrunch.com/2018/03/06/hq-trivia-questions/)_.\n\n_Mobile trivia apps_, such as HQ Trivia and The Q, modernize game show-style entertainment, allowing anyone to try their hand at winning cash prizes from their smartphone from anywhere. However, these applications are susceptible to cheaters and bots. For example, [a developer by the name of Toby Mellor](https://medium.com/@tobymellor/hq-trivia-using-bots-to-win-money-from-online-game-shows-ce2a1b11828b) recently demonstrated how he created a near-realtime system to cheat on HQ Trivia. His post (linked above) details his use of Google cloud APIs to create this system.\n\nWhile the demo was certainly impressive, I felt that I could improve on this system as a proof-of-concept. Armed with his post, I set out to make a better version of his system. However, I set some constraints on myself to make this more of a challenge:\n\n- The system must run offline, with the exception of the QAS (question-and-answer system).\n- The system allow a human enough time to tap in the answer.\n- The system should require minimal human intervention, since a human will be using the system in real time.\n- The system must be functional within one hour.\n\n## Creating a Trivia-Cheating System\n\n### Design\n\nInstead of doing fancy (and slow) network-request hijacking using `mitmproxy`, I decided to run OCR at intervals on a portion of the image coming from the phone and pipe that to Google to provide a quick answer for the human operator to tap in. The system looks something like the following:\n\n\u003cImage\n  src=\"/post-images/trivia-diagram-2.png\"\n  width={700}\n  height={85}\n  className=\"mx-auto\"\n  layout=\"responsive\"\n  alt=\"trivia diagram\"\n/\u003e\n\nTL;DR: a region of the iPhone's screen will get captured, put through OCR, and then searched up on Google to get an answer.\n\n### Implementation\n\n#### Capturing the Screen\n\nFirst, I needed a way to capture only a region of the QuickTime window on my Mac into a temp file. That turned out to be trivial using the excellent CLI `screencapture` tool.\n\nI initially used interactive selection, where the user manually makes a rectangular selection to capture:\n\n```bash\nscreencapture -ci screens.png\n```\n\nHowever, I quickly realized that was extremely inefficient and that hardcoding in rect coordinates (a feature `screencapture` supports) was a much better option:\n\n```bash\n screencapture -Rx,y,w,h screens.png\n```\n\n_Note:_ The rect coordinates are in the form `x,y,w,h`, with `(x, y)` being the coordinate of the top left corner.\n\n#### Optical Character Recognition\n\nIt was time to find a passable OCR system that was fast and accurate. The only one I could think of was [`tesseract`](https://github.com/tesseract-ocr/tesseract), an OCR engine maintained by Google. While not the best system in the world, its functionality out of the box was pretty great in my testing.\n\nIn order to extract text from our image using Tesseract, I used the following,\n\n```bash\ntesseract screens.png ocr\n```\n\nwhich takes the image and puts the extracted text into a file, `ocr.txt`. Now, it's finally time to answer this question.\n\n#### Constructing a Google Search URL and Launching Chrome\n\nOne little nasty step we have to do is to put our URL-encoded search query into a Google Search URL and then push that URL to a browser of choice (I used Google Chrome). I used Python for this step because of its familiarity.\n\n#### URL Encoding the Search Query\n\nA typical Google search URL looks something like this:\n\n\u003e _https://www.google.com/search\u0026q=what+is+life%3F_\n\nNotice how the search query is a query string, the spaces are replaced with `+` signs, and special characters are encoded. Python 3 can do this easily with `urllib.parse.quote_plus(str)`.\n\nSo our code might look something like this:\n\n```python\nquery_encoded = urllib.parse.quote_plus(query)\n```\n\n#### Programmatically Launching Chrome\n\nNow, we need to pass the URL to Chrome programmatically. With a little bit of trial and error, I found out that the Unix binary, located at `/Applications/Google\\ Chrome.app/Contents/MacOS/Google\\ Chrome`, can open URLs without launching a new browser instance. Using that to our advantage, we simply pass the URL as a shell argument, and _violà!_ It works.\n\n#### Stitching These Components Together\n\nNow, it's time to mesh these components together. I decided to keep it simple and created a shell script to run each component and pass around data. My shell script ended up looking like this:\n\n\u003cMultiCode\n  tabs={[\n    {\n      name: \"realtime-ocr.bash\",\n      content: `#!/usr/local/bin/zsh\necho \"Realtime Screen OCR\"\nwhile true\ndo\n    echo \"Waiting for trigger\"\n    read\n    screencapture -Rx,y,w,h screens.png\n    tesseract screens.png ocr\n    OCR=\\`cat ocr.txt\\`\n    python3 launch.py $OCR\n    echo \"Opened Chrome...waiting for next question\"\ndone`,\n      language: \"bash\",\n    },\n  ]}\n/\u003e\n\nThe script waits for the enter key, grabs the predefined rect, runs OCR on it, and then launches Chrome. Then, it goes back to waiting for the enter key.\n\n### Testing and Debrief\n\nI ran tests on this system using video replays of previous HQ Trivia shows. It worked well but struggled on the more difficult questions. Here's a video of the system providing an instant answer to a question:\n\n\u003ccenter\u003e\n  \u003ciframe\n    width=\"560\"\n    height=\"315\"\n    src=\"https://www.youtube-nocookie.com/embed/F1m6ZyqIvhs?rel=0\"\n    frameborder=\"0\"\n    allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\"\n    allowfullscreen\n  \u003e\u003c/iframe\u003e\n\u003c/center\u003e\n\nIn my opinion, this checks all the boxes–it works offline, it's fast, and it's pretty much self-sufficient.\n\nHowever, it's not always 100% accurate. This is because I didn't develop an _actual_ QAS system that breaks down sentence structure using NLP and then searches for detected entities within a corpus. But this shows that _it's possible to build something like this._ This system took me one hour to build from planning to completion; somebody with more time and dedication could build something even more accurate and scam the companies behind these apps for thousands of dollars.\n\n## Defending Against Cheaters\n\nPreventing people from cheating, especially on devices they own, is extremely difficult. [It's also quite difficult to do right.](https://en.wikipedia.org/wiki/Sony_BMG_copy_protection_rootkit_scandal) The dev mentioned in the Introduction, Toby Mellor, suggests a software check for screen capture devices. While a good first step, this doesn't really prevent alternative methods of screen capture (such as just straight-up filming the screen with another camera). I suggest _fooling the underlying algorithms underneath_.\n\n\"How do you do this\", you may ask? The answer is _adversarial perturbations_, a term coined by Evtimov et al. in the paper _[Robust Physical-World Attacks on Deep Learning Models](https://arxiv.org/abs/1707.08945)_. By making the text harder to be read by a bot, you are esssentially rate-limiting cheating. So how would this look like in practice? Maybe something like this, which completely fooled Tesseract:\n\n\u003cImage\n  src=\"/post-images/hq-fooled.png\"\n  width={400}\n  height={500}\n  className=\"mx-auto\"\n/\u003e\n\n(Well, not exactly that. That picture is quite horrible aesthetically)\n\nHQ needs to take advantage of its animated aesthetic and add a pattern, different fonts, particle effects, or something similar to confuse OCR systems.\n\nThere's already been research into how to fool OCR systems, such as this [OCR-resistant font](https://walkerart.org/magazine/sang-mun-defiant-typeface-nsa-privacy), but I think that with the advent of deep learning, nothing short of overlaying/blending unpredictable patterns will prevent determined attackers from developing systems to cheat.\n\n[Discuss on Hacker News](https://news.ycombinator.com/item?id=15944171)\n","data":{"title":"Protecting Trivia Apps from Cheaters","description":"How people cheat at trivia games, and how the companies running them could stop this from happening.","displayDate":"December 16, 2017","date":1513472400000,"tags":["Legacy Content"]},"filePath":"protect-trivia-from-bots.mdx"}]},"__N_SSG":true},"page":"/writing","query":{},"buildId":"3ceUdyat16U65wocjUq8G","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>